{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer-BBC.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/I/HQg7E04YxT+r8OdOJ/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravind-gk/Deep-Learning-for-NLP-Assignments/blob/main/Assignment-5/Transformer_BBC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GAkCEY5x-pX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53949bd-9b3f-44e8-afec-eb0a40281219"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import string\n",
        "import re\n",
        "from tensorflow.keras import models,layers,Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EthxyoG_yILF"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrlcHDsqyKgZ"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Datasets/DL-NLP-A4/train.csv\")\n",
        "# df_test = pd.read_csv(\"/content/drive/MyDrive/Datasets/DL-NLP-A4/test.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/Datasets/DL-NLP-A4/Assignment4_TestLabels.csv\")\n",
        "df_test = df_test[['Text', 'Label - (business, tech, politics, sport, entertainment)']]\n",
        "df_test.columns = ['Text', 'Category']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJuAyPEt04dC"
      },
      "source": [
        "# swap labels with integers\n",
        "df.Category=df.Category.replace(\"business\",0)\n",
        "df.Category=df.Category.replace(\"tech\",1)\n",
        "df.Category=df.Category.replace(\"entertainment\",2)\n",
        "df.Category=df.Category.replace(\"sport\",3)\n",
        "df.Category=df.Category.replace(\"politics\",4)\n",
        "\n",
        "df_test.Category = df_test.Category.replace(\"business\",0)\n",
        "df_test.Category = df_test.Category.replace(\"tech\",1)\n",
        "df_test.Category = df_test.Category.replace(\"entertainment\",2)\n",
        "df_test.Category = df_test.Category.replace(\"sport\",3)\n",
        "df_test.Category = df_test.Category.replace(\"politics\",4)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "SZwe9ajZzwzS",
        "outputId": "88f58410-b506-4b4c-e055-7028e33bf27d"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>german business confidence slides german busin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Category\n",
              "0  worldcom ex-boss launches defence lawyers defe...         0\n",
              "1  german business confidence slides german busin...         0\n",
              "2  bbc poll indicates economic gloom citizens in ...         0\n",
              "3  lifestyle  governs mobile choice  faster  bett...         1\n",
              "4  enron bosses in $168m payout eighteen former e...         0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "JuGiwZhTzyFU",
        "outputId": "180aa863-f587-471d-e139-3199fca940b6"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>qpr keeper day heads for preston queens park r...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>software watching while you work software that...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d arcy injury adds to ireland woe gordon d arc...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>india s reliance family feud heats up the ongo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>boro suffer morrison injury blow middlesbrough...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Category\n",
              "0  qpr keeper day heads for preston queens park r...         3\n",
              "1  software watching while you work software that...         1\n",
              "2  d arcy injury adds to ireland woe gordon d arc...         3\n",
              "3  india s reliance family feud heats up the ongo...         0\n",
              "4  boro suffer morrison injury blow middlesbrough...         3"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mUBWCvH1l2j",
        "outputId": "526a7ed0-6837-440b-91c0-8393187f3d81"
      },
      "source": [
        "print(df.shape)\n",
        "print(df_test.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1490, 2)\n",
            "(735, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUSwKCJN3k_W"
      },
      "source": [
        "# Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20uy1Fvj0gIj"
      },
      "source": [
        "# Functions for cleaning the data\n",
        "def is_special(text):\n",
        "    rem = ''\n",
        "    for i in text: \n",
        "        if i.isalnum():\n",
        "            rem = rem + i\n",
        "        else:\n",
        "            rem = rem + ' '\n",
        "            rem=rem+i\n",
        "            rem = rem + ' '\n",
        "    return rem\n",
        "\n",
        "def rem_extra(text):\n",
        "    rem=re.sub(' +', ' ',text)\n",
        "    return str(rem).strip()\n",
        "\n",
        "def get_max_length(df):\n",
        "    max_length = 0\n",
        "    length=[]\n",
        "    for row in df['Text']:\n",
        "        length.append(len(row.split(\" \")))\n",
        "        if len(row.split(\" \")) > max_length:\n",
        "            max_length = len(row.split(\" \"))\n",
        "    return max_length,length\n",
        "    \n",
        "df.Text=df.Text.apply(is_special)\n",
        "df.Text=df.Text.apply(rem_extra)\n",
        "\n",
        "df_test.Text=df_test.Text.apply(is_special)\n",
        "df_test.Text=df_test.Text.apply(rem_extra)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkEvSTW823YM",
        "outputId": "aecfb778-62e0-452b-b98c-3510e94ef787"
      },
      "source": [
        "token=keras.preprocessing.text.Tokenizer(filters='\"#$*+/:;<=>@[\\\\]^_{|}~\\t\\n')\n",
        "token.fit_on_texts(df.Text)\n",
        "size_of_vocabulary=len(token.word_index)+1\n",
        "size_of_vocabulary"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24792"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OgaeeS53De_"
      },
      "source": [
        "def prepare_text(text, token, max_len):\n",
        "    text_seqs = token.texts_to_sequences(text)\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(text_seqs, maxlen = max_len)\n",
        "\n",
        "max_len=1000\n",
        "data_train = prepare_text(df.Text, token, max_len)\n",
        "data_test = prepare_text(df_test.Text, token, max_len)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVEMqIuZ3NpE"
      },
      "source": [
        "X_train = data_train\n",
        "X_test = data_test\n",
        "y_train = df.Category.values\n",
        "y_test = df_test.Category.values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FckmJXoc3nZT"
      },
      "source": [
        "# Load word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlF_I2zn3mbM"
      },
      "source": [
        "def load_emb(addrs,size_of_vocabulary,tk):\n",
        "    embeddings_index = dict()\n",
        "    f = open(addrs)\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "    \n",
        "    embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
        "    c=0\n",
        "    for word, i in tk.word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            c+=1\n",
        "        else:\n",
        "            pass\n",
        "    print('No. of out of vocab word in train set= %s'%(size_of_vocabulary-c))\n",
        "    return embedding_matrix\n",
        "\n",
        "def w2v():\n",
        "  from gensim import models\n",
        "  w2v = models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Embeddings/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "  embd_w2v = np.zeros((size_of_vocabulary, 300))\n",
        "  c=0\n",
        "  for word, i in token.word_index.items():\n",
        "      try:\n",
        "        embd_w2v[i] =w2v[word]   \n",
        "      except:\n",
        "        c+=1\n",
        "        \n",
        "  print('No. of out of vocab word in train set= %s'%(c))\n",
        "  return embd_w2v"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U80aSbD1325H",
        "outputId": "085b850d-2a5a-454d-c3c7-306406959760"
      },
      "source": [
        "# fasttext = load_emb(\"/content/drive/MyDrive/Embeddings/wiki-news-300d-1M-subword.vec\",size_of_vocabulary,token)\n",
        "glove = load_emb(\"/content/drive/MyDrive/Embeddings/glove.6B.300d.txt\",size_of_vocabulary,token)\n",
        "# word2vec = w2v()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors.\n",
            "No. of out of vocab word in train set= 1053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoAdqi2n4hgb"
      },
      "source": [
        "# Define the transformer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juG4z_qo4cLa"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs_qu9rK4r1o"
      },
      "source": [
        "# Without positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMFbZGeK4riG"
      },
      "source": [
        "# without positional embeddings\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, weight_matrix, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, weights = [weight_matrix], trainable = False)\n",
        "        # self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        # maxlen = tf.shape(x)[-1]\n",
        "        # positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        # positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        # return x + positions\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHfFn3bo48vP"
      },
      "source": [
        "embed_dim = 300  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "vocab_size = size_of_vocabulary\n",
        "\n",
        "inputs = layers.Input(shape=(max_len,))\n",
        "embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, glove, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W25Rfcy5ozm",
        "outputId": "99aabf15-99a7-4e0b-f3cc-486e1aa11245"
      },
      "source": [
        "# using glove, without position embeddings\n",
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test)\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "24/24 [==============================] - 21s 705ms/step - loss: 1.5640 - accuracy: 0.3148 - val_loss: 1.2741 - val_accuracy: 0.4898\n",
            "Epoch 2/10\n",
            "24/24 [==============================] - 17s 695ms/step - loss: 1.1845 - accuracy: 0.4638 - val_loss: 0.9786 - val_accuracy: 0.5551\n",
            "Epoch 3/10\n",
            "24/24 [==============================] - 17s 700ms/step - loss: 0.9285 - accuracy: 0.5711 - val_loss: 0.7451 - val_accuracy: 0.6422\n",
            "Epoch 4/10\n",
            "24/24 [==============================] - 17s 704ms/step - loss: 0.6532 - accuracy: 0.7450 - val_loss: 0.5008 - val_accuracy: 0.7837\n",
            "Epoch 5/10\n",
            "24/24 [==============================] - 17s 706ms/step - loss: 0.2916 - accuracy: 0.9027 - val_loss: 0.2942 - val_accuracy: 0.9524\n",
            "Epoch 6/10\n",
            "24/24 [==============================] - 17s 706ms/step - loss: 0.1505 - accuracy: 0.9604 - val_loss: 0.2106 - val_accuracy: 0.9551\n",
            "Epoch 7/10\n",
            "24/24 [==============================] - 17s 706ms/step - loss: 0.0795 - accuracy: 0.9805 - val_loss: 0.1895 - val_accuracy: 0.9605\n",
            "Epoch 8/10\n",
            "24/24 [==============================] - 17s 707ms/step - loss: 0.0487 - accuracy: 0.9899 - val_loss: 0.1863 - val_accuracy: 0.9619\n",
            "Epoch 9/10\n",
            "24/24 [==============================] - 17s 708ms/step - loss: 0.0449 - accuracy: 0.9879 - val_loss: 0.2257 - val_accuracy: 0.9578\n",
            "Epoch 10/10\n",
            "24/24 [==============================] - 17s 707ms/step - loss: 0.0416 - accuracy: 0.9893 - val_loss: 0.2073 - val_accuracy: 0.9646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXJ2np9Y8HsB",
        "outputId": "f349bc72-7acf-41bd-d784-e51564bfa381"
      },
      "source": [
        "y_pred = model.predict(data_test)\n",
        "print(classification_report(y_test, np.argmax(y_pred, axis=1)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.94       176\n",
            "           1       0.94      0.98      0.96       135\n",
            "           2       0.96      0.98      0.97       113\n",
            "           3       1.00      0.97      0.98       168\n",
            "           4       0.96      0.98      0.97       143\n",
            "\n",
            "    accuracy                           0.96       735\n",
            "   macro avg       0.96      0.97      0.96       735\n",
            "weighted avg       0.97      0.96      0.96       735\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emJkkXoK8lX0",
        "outputId": "2bca73ed-c8d7-48ba-f911-4d599ff05db4"
      },
      "source": [
        "model.save(filepath = '/content/drive/MyDrive/Models/DL-NLP-A5/transformer-BBC-withoutPE')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models/DL-NLP-A5/transformer-BBC-withoutPE/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models/DL-NLP-A5/transformer-BBC-withoutPE/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxe_3DpG6Z1m"
      },
      "source": [
        "# With positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN5QSbsB6dXg"
      },
      "source": [
        "# with positional embeddings\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, weight_matrix, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, weights = [weight_matrix], trainable = False)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "        return x"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VraU_S_s6g6W"
      },
      "source": [
        "embed_dim = 300  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "vocab_size = size_of_vocabulary\n",
        "\n",
        "inputs = layers.Input(shape=(max_len,))\n",
        "embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, glove, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNdcWe2I6kqk",
        "outputId": "0babd616-e09a-45eb-e906-a67abf04ab4c"
      },
      "source": [
        "# using glove, without position embeddings\n",
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    X_train, y_train, batch_size=64, epochs=10, validation_data=(X_test, y_test)\n",
        ")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "24/24 [==============================] - 20s 769ms/step - loss: 1.5974 - accuracy: 0.3020 - val_loss: 1.3878 - val_accuracy: 0.4980\n",
            "Epoch 2/10\n",
            "24/24 [==============================] - 18s 771ms/step - loss: 1.0419 - accuracy: 0.6289 - val_loss: 0.6959 - val_accuracy: 0.8000\n",
            "Epoch 3/10\n",
            "24/24 [==============================] - 18s 773ms/step - loss: 0.5202 - accuracy: 0.7490 - val_loss: 0.3611 - val_accuracy: 0.9415\n",
            "Epoch 4/10\n",
            "24/24 [==============================] - 19s 777ms/step - loss: 0.2080 - accuracy: 0.9409 - val_loss: 0.2133 - val_accuracy: 0.9510\n",
            "Epoch 5/10\n",
            "24/24 [==============================] - 19s 779ms/step - loss: 0.0834 - accuracy: 0.9752 - val_loss: 0.2183 - val_accuracy: 0.9551\n",
            "Epoch 6/10\n",
            "24/24 [==============================] - 19s 781ms/step - loss: 0.0664 - accuracy: 0.9758 - val_loss: 0.2186 - val_accuracy: 0.9605\n",
            "Epoch 7/10\n",
            "24/24 [==============================] - 19s 779ms/step - loss: 0.0349 - accuracy: 0.9913 - val_loss: 0.2135 - val_accuracy: 0.9592\n",
            "Epoch 8/10\n",
            "24/24 [==============================] - 19s 779ms/step - loss: 0.0457 - accuracy: 0.9852 - val_loss: 0.2127 - val_accuracy: 0.9524\n",
            "Epoch 9/10\n",
            "24/24 [==============================] - 19s 778ms/step - loss: 0.0259 - accuracy: 0.9933 - val_loss: 0.2083 - val_accuracy: 0.9633\n",
            "Epoch 10/10\n",
            "24/24 [==============================] - 19s 778ms/step - loss: 0.0291 - accuracy: 0.9933 - val_loss: 0.2329 - val_accuracy: 0.9619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irDh722BB28V",
        "outputId": "913d9ff1-095d-4918-a6e6-d2a649835d48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = model.predict(data_test)\n",
        "print(classification_report(y_test, np.argmax(y_pred, axis=1)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.94      0.94       176\n",
            "           1       0.94      0.99      0.96       135\n",
            "           2       0.96      0.95      0.95       113\n",
            "           3       1.00      0.98      0.99       168\n",
            "           4       0.97      0.95      0.96       143\n",
            "\n",
            "    accuracy                           0.96       735\n",
            "   macro avg       0.96      0.96      0.96       735\n",
            "weighted avg       0.96      0.96      0.96       735\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02z8P62hB9qF",
        "outputId": "486a8483-f397-4754-84d2-61803cd6de2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.save(filepath = '/content/drive/MyDrive/Models/DL-NLP-A5/transformer-BBC-withPE')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_3_layer_call_fn, embedding_3_layer_call_and_return_conditional_losses, embedding_4_layer_call_fn, embedding_4_layer_call_and_return_conditional_losses, multi_head_attention_2_layer_call_fn while saving (showing 5 of 65). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models/DL-NLP-A5/transformer-BBC-withPE/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Models/DL-NLP-A5/transformer-BBC-withPE/assets\n"
          ]
        }
      ]
    }
  ]
}