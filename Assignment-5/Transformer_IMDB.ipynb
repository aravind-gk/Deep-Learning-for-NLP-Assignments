{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers-IMDB.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNlhg2ci87ZQlR3FFbbmydg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravind-gk/Deep-Learning-for-NLP-Assignments/blob/main/Assignment-5/Transformer_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47UWLoX2I1Er",
        "outputId": "c9d050fd-931d-4e25-fbff-c34d058609cf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import string\n",
        "import re\n",
        "from tensorflow.keras import models,layers,Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMUKiVevU3QH"
      },
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct1TiQUUVcMq",
        "outputId": "601292fd-4d3a-4cc5-d7ad-e1f738d4c6ab"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08jaWB7hJIdr"
      },
      "source": [
        "Fetch data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyYzz8OgJKUt"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Datasets/DL-NLP-A5/IMDB.csv\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb94jW8ZT1Su"
      },
      "source": [
        "Clean the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38cZgl0wVBYM"
      },
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cgnCtzQT3S1"
      },
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "#Removing the stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
        "            final_text.append(i.strip().lower())\n",
        "    return \" \".join(final_text)\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df['review']=df['review'].apply(denoise_text)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtM9qJOdT4RU"
      },
      "source": [
        "Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO-WbHRFVybX"
      },
      "source": [
        "df_train = df.iloc[:25000]\n",
        "df_test = df.iloc[25000:]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zWG2riVJQC4",
        "outputId": "0dabcd76-084d-492c-e8dc-946545e11fde"
      },
      "source": [
        "tk = keras.preprocessing.text.Tokenizer(filters='\"#$*+/:;<=>@[\\\\]^_{|}~\\t\\n')\n",
        "tk.fit_on_texts(df_train.review)\n",
        "\n",
        "def prep_text(txt, tk, max_seq_len):\n",
        "    text_seqs = tk.texts_to_sequences(txt)\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(text_seqs, maxlen=max_seq_len)\n",
        "\n",
        "data_train = prep_text(df_train.review, tk,60)\n",
        "data_test = prep_text(df_test.review, tk,60)\n",
        "\n",
        "size_of_vocabulary=len(tk.word_index)+1\n",
        "size_of_vocabulary"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63152"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItXed43hMCX5",
        "outputId": "169be908-ef71-4e20-fe45-b6a45feada25"
      },
      "source": [
        "data_train = keras.preprocessing.sequence.pad_sequences(data_train, maxlen=maxlen)\n",
        "data_test = keras.preprocessing.sequence.pad_sequences(data_test, maxlen=maxlen)\n",
        "dic = {'positive':1, 'negative':0}\n",
        "df_train['sentiment'] = df_train['sentiment'].replace(dic)\n",
        "df_test['sentiment'] = df_test['sentiment'].replace(dic)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYFOA8aWL2fO"
      },
      "source": [
        "Load embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVM2ACVnKM3R",
        "outputId": "cc10ce59-0817-40d9-cdc2-1cd3ba2a8691"
      },
      "source": [
        "## Load pretrained datasets\n",
        "# load the whole embedding into memory\n",
        "def load_Mat(addrs,size_of_vocabulary,tk):\n",
        "    embeddings_index = dict()\n",
        "    f = open(addrs)\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "    f.close()\n",
        "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "    \n",
        "    # create a weight matrix for words in training docs\n",
        "    embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
        "    c=0\n",
        "    for word, i in tk.word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            c+=1\n",
        "        else:\n",
        "            #print(word)\n",
        "            pass\n",
        "    print('No. of out of vocab word in train set= %s'%(size_of_vocabulary-c))\n",
        "    \n",
        "    return embedding_matrix\n",
        "\n",
        "glove=load_Mat(\"/content/drive/MyDrive/Embeddings/glove.6B.300d.txt\",size_of_vocabulary,tk)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors.\n",
            "No. of out of vocab word in train set= 9780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i37D_8gWQCpc"
      },
      "source": [
        "# Reproducing previous results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWReKGO4QGC0"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUUiGxKzRBtP"
      },
      "source": [
        "## Without positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LJdhkpLQNJK"
      },
      "source": [
        "# without positional embeddings\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, weight_matrix, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, weights = [weight_matrix], trainable = False)\n",
        "        # self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        # maxlen = tf.shape(x)[-1]\n",
        "        # positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        # positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        # return x + positions\n",
        "        return x"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Lr1CEUQYzf"
      },
      "source": [
        "\n",
        "embed_dim = 300  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "maxlen = 200\n",
        "vocab_size = size_of_vocabulary\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, glove, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBQRckILQeiX",
        "outputId": "3e60082c-c52f-43bc-ee46-5eb6cea26551"
      },
      "source": [
        "# using glove, without position embeddings\n",
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    data_train, df_train.sentiment.values, batch_size=32, epochs=2, validation_data=(data_test, df_test.sentiment.values)\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 55s 68ms/step - loss: 0.5848 - accuracy: 0.6735 - val_loss: 0.4923 - val_accuracy: 0.7604\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 54s 69ms/step - loss: 0.4950 - accuracy: 0.7595 - val_loss: 0.4737 - val_accuracy: 0.7739\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGi9Q_bQXF4U",
        "outputId": "db2645e0-5d22-4ae2-e38b-cca563c61a87"
      },
      "source": [
        "# using glove, without position embeddings, after cleaning\n",
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    data_train, df_train.sentiment.values, batch_size=32, epochs=5, validation_data=(data_test, df_test.sentiment.values)\n",
        ")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 56s 69ms/step - loss: 0.4678 - accuracy: 0.7831 - val_loss: 0.4658 - val_accuracy: 0.7902\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.4144 - accuracy: 0.8181 - val_loss: 0.4051 - val_accuracy: 0.8176\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.3956 - accuracy: 0.8286 - val_loss: 0.3982 - val_accuracy: 0.8179\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.3809 - accuracy: 0.8336 - val_loss: 0.3801 - val_accuracy: 0.8300\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 53s 68ms/step - loss: 0.3733 - accuracy: 0.8376 - val_loss: 0.3836 - val_accuracy: 0.8264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATLqHN0_RFTj"
      },
      "source": [
        "## With positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfN8rKhmRIyQ"
      },
      "source": [
        "# with positional embeddings\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, weight_matrix, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, weights = [weight_matrix], trainable = False)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "        return x"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgaDuoboROWt"
      },
      "source": [
        "\n",
        "embed_dim = 300  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "maxlen = 200\n",
        "vocab_size = size_of_vocabulary\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, glove, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNnt28MhRQg3",
        "outputId": "2942af50-a6bb-423f-e582-b8961cadc50e"
      },
      "source": [
        "# using glove, with position embeddings\n",
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    data_train, df_train.sentiment.values, batch_size=32, epochs=2, validation_data=(data_test, df_test.sentiment.values)\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 62s 78ms/step - loss: 0.5518 - accuracy: 0.7135 - val_loss: 0.4758 - val_accuracy: 0.7701\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 60s 77ms/step - loss: 0.4862 - accuracy: 0.7682 - val_loss: 0.5021 - val_accuracy: 0.7330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nitUkarGR8r1",
        "outputId": "d0781589-2f3b-4f23-f615-e7fff651359e"
      },
      "source": [
        "history = model.fit(\n",
        "    data_train, df_train.sentiment.values, batch_size=32, epochs=10, validation_data=(data_test, df_test.sentiment.values)\n",
        ")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 60s 77ms/step - loss: 0.4635 - accuracy: 0.7794 - val_loss: 0.4543 - val_accuracy: 0.7884\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.4490 - accuracy: 0.7895 - val_loss: 0.4585 - val_accuracy: 0.7833\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.4363 - accuracy: 0.7960 - val_loss: 0.4447 - val_accuracy: 0.7910\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.4259 - accuracy: 0.7996 - val_loss: 0.4621 - val_accuracy: 0.7800\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 60s 77ms/step - loss: 0.4172 - accuracy: 0.8072 - val_loss: 0.4551 - val_accuracy: 0.7854\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.4104 - accuracy: 0.8079 - val_loss: 0.4485 - val_accuracy: 0.7859\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.4025 - accuracy: 0.8124 - val_loss: 0.4522 - val_accuracy: 0.7874\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.3943 - accuracy: 0.8149 - val_loss: 0.4676 - val_accuracy: 0.7898\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.3874 - accuracy: 0.8210 - val_loss: 0.4585 - val_accuracy: 0.7855\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.3807 - accuracy: 0.8251 - val_loss: 0.4741 - val_accuracy: 0.7824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh8QLAEDYmCV",
        "outputId": "e50aa4bf-3d36-4a3d-87c8-f637fdae8c3c"
      },
      "source": [
        "# using glove, with position embeddings, cleaned data\n",
        "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    data_train, df_train.sentiment.values, batch_size=32, epochs=10, validation_data=(data_test, df_test.sentiment.values)\n",
        ")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 62s 77ms/step - loss: 0.4680 - accuracy: 0.7722 - val_loss: 0.4313 - val_accuracy: 0.7856\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.4028 - accuracy: 0.8209 - val_loss: 0.3826 - val_accuracy: 0.8256\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.3861 - accuracy: 0.8282 - val_loss: 0.3844 - val_accuracy: 0.8295\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 59s 76ms/step - loss: 0.3764 - accuracy: 0.8337 - val_loss: 0.3761 - val_accuracy: 0.8267\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.3679 - accuracy: 0.8377 - val_loss: 0.3774 - val_accuracy: 0.8285\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 60s 77ms/step - loss: 0.3563 - accuracy: 0.8430 - val_loss: 0.3813 - val_accuracy: 0.8292\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 60s 77ms/step - loss: 0.3530 - accuracy: 0.8431 - val_loss: 0.3811 - val_accuracy: 0.8284\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 60s 77ms/step - loss: 0.3419 - accuracy: 0.8492 - val_loss: 0.3827 - val_accuracy: 0.8278\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 60s 76ms/step - loss: 0.3342 - accuracy: 0.8533 - val_loss: 0.3844 - val_accuracy: 0.8252\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 60s 77ms/step - loss: 0.3270 - accuracy: 0.8590 - val_loss: 0.3920 - val_accuracy: 0.8222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd1kOM7WTwT7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpzmFCk_TvhY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}