{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "colab": {
      "name": "BERT-IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aravind-gk/Deep-Learning-for-NLP-Assignments/blob/main/Assignment-5/BERT_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_kg_hide-output": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "iQUP9Q_-6QDv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras \n",
        "from keras import backend as K\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import LSTM,Dense,Bidirectional,Input\n",
        "from keras.models import Model\n",
        "import torch\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYBJo_lzDXjr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Datasets/DL-NLP-A5/IMDB.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GQtuDAL6QDx"
      },
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3GZy7At6QDy"
      },
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n",
        "            final_text.append(i.strip().lower())\n",
        "    return \" \".join(final_text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "df['review']=df['review'].apply(denoise_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTI-eM1e6QDy",
        "outputId": "75c13ed7-7264-4453-9e52-838a950ac0a8"
      },
      "source": [
        "df.sentiment.replace(\"positive\" , 1 , inplace = True)\n",
        "df.sentiment.replace(\"negative\" , 0 , inplace = True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one reviewers mentioned watching oz episode ex...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wonderful little filming technique fashion giv...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thought wonderful way spend time hot summer si...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically family little boy thinks zombie clos...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter time visually stunning film mattei offe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  one reviewers mentioned watching oz episode ex...          1\n",
              "1  wonderful little filming technique fashion giv...          1\n",
              "2  thought wonderful way spend time hot summer si...          1\n",
              "3  basically family little boy thinks zombie clos...          0\n",
              "4  petter time visually stunning film mattei offe...          1"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR3DnBzYENHQ"
      },
      "source": [
        "df_train = df.iloc[:25000]\n",
        "df_test = df.iloc[25000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32dNQmMW6QD3"
      },
      "source": [
        "# TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXycU1TcHFR-"
      },
      "source": [
        "x_train = df_train.review\n",
        "x_test = df_test.review\n",
        "y_train = df_train.sentiment\n",
        "y_test = df_train.sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJmQgg856QD3"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(df.review,df.sentiment,random_state = 0 , stratify = df.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a89fef190ea2400ca5a531ec93babf86"
          ]
        },
        "id": "hDM8OJnz6QD4",
        "outputId": "aeef593f-7696-4ef2-b701-4ac0c0e01b7c"
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower = True)\n",
        "tokenizer.save_pretrained('./vocab.txt')\n",
        "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\n",
        "fast_tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a89fef190ea2400ca5a531ec93babf86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=True, wordpieces_prefix=##)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djgd4HMP6QD5"
      },
      "source": [
        "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=400):\n",
        "\n",
        "    tokenizer.enable_truncation(max_length=maxlen)\n",
        "    tokenizer.enable_padding(max_length=maxlen)\n",
        "    all_ids = []\n",
        "    \n",
        "    for i in range(0, len(texts), chunk_size):\n",
        "        text_chunk = texts[i:i+chunk_size].tolist()\n",
        "        encs = tokenizer.encode_batch(text_chunk)\n",
        "        all_ids.extend([enc.ids for enc in encs])\n",
        "    \n",
        "    return np.array(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTy1m_Mz6QD5"
      },
      "source": [
        "x_train = fast_encode(x_train.values, fast_tokenizer, maxlen=400)\n",
        "x_test = fast_encode(x_test.values, fast_tokenizer, maxlen=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw0JrvjP6QD5"
      },
      "source": [
        "def build_model(transformer, max_len=400):\n",
        "    \n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "73f0f1f727894b6b856bd491daea2f59",
            "19e416c4999f4be682d7e566924e05e8"
          ]
        },
        "id": "tolIWmLU6QD5",
        "outputId": "c87fd1cc-c90d-4077-c1f2-5a7bd075b7ad"
      },
      "source": [
        "bert_model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73f0f1f727894b6b856bd491daea2f59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19e416c4999f4be682d7e566924e05e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq7Al8HL6QD5"
      },
      "source": [
        "model = build_model(bert_model, max_len=400)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFZ_rfYv6QD6",
        "outputId": "5367f956-5caf-4f15-8e12-6a8786d0394e"
      },
      "source": [
        "history = model.fit(x_train,y_train,batch_size = 16 ,validation_data=(x_test,y_test),epochs = 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "1172/1172 [==============================] - 1018s 869ms/step - loss: 0.3522 - accuracy: 0.8400 - val_loss: 0.2794 - val_accuracy: 0.8811\n",
            "Epoch 2/3\n",
            "1172/1172 [==============================] - 1016s 867ms/step - loss: 0.2379 - accuracy: 0.9033 - val_loss: 0.3428 - val_accuracy: 0.8526\n",
            "Epoch 3/3\n",
            "1172/1172 [==============================] - 1015s 866ms/step - loss: 0.1494 - accuracy: 0.9433 - val_loss: 0.3069 - val_accuracy: 0.8855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1Qi4KmW6QD6"
      },
      "source": [
        "# ANALYSIS AFTER TRAINING OF MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfjogwaO6QD6",
        "outputId": "a03eaa26-3204-42ba-acad-9d52cc867b8f"
      },
      "source": [
        "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 99s 253ms/step - loss: 0.3069 - accuracy: 0.8855\n",
            "Accuracy of the model on Testing Data is -  88.55199813842773 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHrERXaL6QD6"
      },
      "source": [
        "pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtERrSK-6QD6"
      },
      "source": [
        "pred = np.round(pred).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZChUw-16QD6",
        "outputId": "f0c1e60f-4669-43f1-cee8-bb0af49ec59f"
      },
      "source": [
        "print(classification_report(y_test, pred, target_names = ['Bad Reviews','Good Reviews']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " Bad Reviews       0.91      0.86      0.88      6250\n",
            "Good Reviews       0.86      0.91      0.89      6250\n",
            "\n",
            "    accuracy                           0.89     12500\n",
            "   macro avg       0.89      0.89      0.89     12500\n",
            "weighted avg       0.89      0.89      0.89     12500\n",
            "\n"
          ]
        }
      ]
    }
  ]
}